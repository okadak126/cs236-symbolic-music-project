<<<<<<< HEAD
# Symbolic Music Generation with Diffusion Models
Supplementary code release.

## Installation
All code is written in Python 3 ([Anaconda](https://www.anaconda.com/) recommended). To install the dependencies:

```
pip install -r requirements.txt
```

A copy of the [Magenta](https://github.com/magenta/magenta) codebase is required for access to MusicVAE and related components. [Installation instructions](https://github.com/magenta/magenta#installation) can be found on the Magenta public repository. You will also need to download [pretrained MusicVAE checkpoints](https://github.com/magenta/magenta/tree/master/magenta/models/music_vae). For our experiments, we use the [2-bar melody model](https://storage.googleapis.com/magentadata/models/music_vae/checkpoints/cat-mel_2bar_big.tar).

## Datasets
We use the [Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/) to train our models. Follow [these instructions](https://github.com/magenta/magenta/blob/master/magenta/scripts/README.md) to download and build the Lakh MIDI Dataset.


To encode the Lakh dataset with MusicVAE, use `scripts/generate_song_data_beam.py`:
```
python scripts/generate_song_data_beam.py \
  --checkpoint=/path/to/musicvae-ckpt \
  --input=/path/to/lakh_tfrecords \
  --output=/path/to/encoded_tfrecords
``` 

To preprocess and generate fixed-length latent sequences for training diffusion and autoregressive models, refer to `scripts/transform_encoded_data.py`:
```
python scripts/transform_encoded_data.py \
  --encoded_data=/path/to/encoded_tfrecords \
  --output_path =/path/to/preprocess_tfrecords \
  --mode=sequences \
  --context_length=32
```
## Training
#### Diffusion
```python train_ncsn.py --flagfile=configs/ddpm-mel-32seq-512.cfg```

#### TransformerMDN
```python train_mdn.py --flagfile=configs/mdn-mel-32seq-512.cfg```

## Sampling and Generation
#### Diffusion
```
python sample_ncsn.py \
  --flagfile=configs/ddpm-mel-32seq-512.cfg \
  --sample_seed=42 \
  --sample_size=1000 \
  --sampling_dir=/path/to/latent-samples 
```

#### TransformerMDN
```
python sample_ncsn.py \
  --flagfile=configs/mdn-mel-32seq-512.cfg \
  --sample_seed=42 \
  --sample_size=1000 \
  --sampling_dir=/path/to/latent-samples 
```

#### Decoding sequences
To convert sequences of embeddings (generated by diffusion or TransformerMDN models) to sequences of MIDI events, refer to `scripts/sample_audio.py`.

```
python scripts/sample_audio.py
  --input=/path/to/latent-samples/[ncsn|mdn] \
  --output=/path/to/audio-midi \
  --n_synth=1000 \
  --include_wav=True
```
=======
# cs236-symbolic-music-project
Final Project for Stanford CS 236
>>>>>>> 3316bd7a60a51dbfb8ae6c5f0fb71190608a2874
